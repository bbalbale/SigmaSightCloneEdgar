# Chat Integration Implementation Plan

**Created:** 2025-01-09  
**Updated:** 2025-09-01  
**Status:** Implementation Phase  
**Target:** SigmaSight Portfolio Chat Assistant  
**Architecture:** Sheet Overlay with fetch() Streaming + HttpOnly Cookies

## 1.0 Version Summary

### 1.1 Current Version (V1.1) - Updated 2025-09-01
Major architectural decisions and feedback integration based on technical review and backend analysis.

**Key Changes from Original (v1.0 - commit 2363b54):**

#### 1.1.1 üîÑ **Major Architectural Decisions**
1. **Streaming Approach**: Changed from EventSource to fetch() POST for better control and JSON payload support
2. **Authentication Model**: Switched from JWT localStorage to HttpOnly cookies for security and SSE compatibility
3. **Backend Integration**: Updated to leverage existing agent backend UUID system (no new backend changes needed)

#### 1.1.2 ‚úÖ **Feedback Integration** 
Added 5 high-priority features from technical review:
- **1.1.2.1**: Split Store Architecture (performance - separate persistent vs runtime state)
- **1.1.2.2**: Client-Side Message Queue (UX - prevent spam, visual feedback)  
- **1.1.2.3**: Mobile Input Handling (mobile UX - keyboard management, safe areas)
- **1.1.2.4**: Deployment Checklist (production - nginx configs, CDN settings)
- **1.1.2.5**: Observability & Debugging (maintainability - trace IDs using existing backend UUIDs)

#### 1.1.3 üéØ **Implementation Readiness**
- **Backend Status**: Upgraded from "95% Complete" to "100% Ready for V1.1"
- **Frontend Status**: Upgraded from "5%" to "25% Complete" (UI components done)
- **Timeline**: Compressed from vague multi-phase to concrete 2-week implementation plan
- **Risk Mitigation**: Solved major failure points (auth expiration, CORS issues, deduplication)

#### 1.1.4 üìã **Scope Clarification**
- **V1.1 Simplifications**: Clear list of features deferred to maintain 2-week timeline
- **Mixed Auth Strategy**: Keep JWT for portfolio pages, cookies for chat (minimize risk)
- **Production Focus**: Added comprehensive deployment considerations

**Original Focus**: Architectural exploration and EventSource streaming  
**Current Focus**: Production-ready implementation with concrete timeline and risk mitigation  

## 2.0 Implementation Decisions (V1.1)

Based on technical analysis and backend capabilities review, we've made two key architectural decisions that prioritize **simplicity, safety, and speed to prototype** for our V1 serving ~50 users:

### 2.1 Streaming Mechanism: fetch() POST

**Decision:** Use `fetch()` with `POST /api/v1/chat/send` for streaming assistant responses.

**Rationale:**
- **Single-call flow**: Send prompt, mode, metadata, or small JSON files in the same request that starts the stream
- **Easy to reason about**: Both users and developers understand "send ‚Üí stream back" flow
- **Future flexibility**: Unlike EventSource, `fetch()` supports POST and custom headers if needed later
- **Clean architecture**: Avoids the two-step EventSource workaround pattern
- **Trade-off accepted**: Slightly more code (manual stream parsing) but keeps implementation straightforward

### 2.2 Authentication Model: HttpOnly Cookies

**Decision:** Use HttpOnly session cookies for all chat endpoints, including streaming.

**Rationale:**
- **Simplified auth flow**: No tokens in localStorage, no JWT headers required for streaming
- **Automatic handling**: Cookies are sent with `fetch()` requests automatically, no special logic needed
- **XSS protection**: JavaScript cannot read HttpOnly cookies, mitigating token theft risks
- **Perfect compatibility**: Works seamlessly with `fetch()` POST approach, no JWT vs cookie juggling
- **V1.1 appropriate**: Simplest and safest option for prototype scale

**Implementation Details:**

**Backend Requirements (Already Supported):**
```python
# Backend already supports dual auth per CLAUDE.md:
# "Auth uses dual support: Both Bearer tokens (existing) AND cookies (for SSE)"

# Cookie configuration in backend response:
Set-Cookie: session_id=abc123; HttpOnly; Secure; SameSite=Strict; Max-Age=86400
```

**Frontend Transition:**
```typescript
// Phase 1: Keep existing JWT for portfolio pages
// Phase 2: Chat endpoints use cookies only
// Phase 3: (Future) Migrate all endpoints to cookies

// Chat login (new approach)
await fetch('/api/proxy/auth/login', {
  method: 'POST',
  credentials: 'include',
  body: JSON.stringify({ email, password })
})

// All chat requests automatically include cookies
await fetch('/api/proxy/chat/send', {
  credentials: 'include' // No Authorization header needed
})
```

#### 2.2.1 Development vs Production:
- **Development**: Cookies work with localhost proxy
- **Production**: Requires proper domain and HTTPS for Secure flag

## 3.0 Executive Summary

Implement a chat interface that overlays the current page using shadcn Sheet component, connecting to the existing backend OpenAI integration via fetch() streaming. The chat provides contextual portfolio analysis while preserving the user's current view.

## 4.0 Current State Analysis

### 4.1 Backend (100% Ready for V1.1)
- ‚úÖ **Database**: `agent_conversations`, `agent_messages` tables with UUID conversation IDs
- ‚úÖ **Authentication**: HttpOnly cookie support ready for streaming endpoints
- ‚úÖ **Chat Endpoints**: 
  - `POST /api/v1/chat/conversations` - Create conversation
  - `GET /api/v1/chat/conversations` - List conversations
  - `DELETE /api/v1/chat/conversations/{id}` - Delete conversation
  - `GET /api/v1/chat/conversations/{id}/messages` - Get history with pagination
  - `POST /api/v1/chat/send` - Send message (fetch() streaming response)
- ‚úÖ **OpenAI Integration**: GPT-4o with 6 portfolio analysis function tools
- ‚úÖ **Conversation Modes**: green/blue/indigo/violet prompt personalities
- ‚úÖ **Streaming Infrastructure**: fetch()-compatible SSE with heartbeats
- ‚úÖ **Deduplication**: Conversation and message UUIDs prevent duplicates

### 4.2 Frontend (25% Complete - UI Done)
- ‚úÖ **Sheet UI**: ChatInterface with Sheet overlay pattern implemented
- ‚úÖ **State Management**: Zustand chatStore with message history
- ‚úÖ **ChatInput Component**: Basic input with send functionality
- ‚úÖ **API Proxy**: `/api/proxy/[...path]` ready for cookie forwarding
- ‚úÖ **Authentication Flow**: Portfolio auth system ready for cookie conversion
- ‚ùå **fetch() Streaming**: Need to implement POST streaming parser
- ‚ùå **Cookie Auth**: Need to switch from localStorage JWT to cookies
- ‚ùå **Message Display**: Need real backend integration

## 5.0 Architecture Decision

### 5.1 UI Pattern: Sheet Overlay
**Rationale:**
- Preserves user context (stays on current page)
- Progressive disclosure (chat bar ‚Üí full conversation)
- Mobile-friendly (bottom sheet on mobile, sidebar on desktop)
- Non-intrusive user experience

### 5.2 Technical Stack (Updated for V1)
- **UI Library**: shadcn/ui Sheet component (already implemented)
- **State Management**: Zustand split stores (chat state + stream state)
- **Streaming**: fetch() POST with manual SSE parsing
- **Authentication**: HttpOnly session cookies (no localStorage)
- **Markdown**: react-markdown with rehype-sanitize
- **Styling**: Tailwind CSS (existing)

## 6.0 Implementation Phases

### 6.1 Phase 1: Sheet UI Infrastructure (Day 1)

#### 1.1 Install Dependencies
```bash
npx shadcn-ui@latest add sheet
npm install zustand react-markdown rehype-sanitize
```

#### 1.2 Create Chat Store
```typescript
// frontend/src/stores/chatStore.ts
interface ChatState {
  // Conversation Management
  conversations: Conversation[]
  currentConversationId: string | null
  messages: Message[]
  
  // UI State
  isSheetOpen: boolean
  isStreaming: boolean
  streamingMessage: string
  mode: ConversationMode
  
  // Actions
  setSheetOpen: (open: boolean) => void
  addMessage: (message: Message) => void
  updateStreamingMessage: (content: string) => void
}
```

#### 1.3 Build Sheet Container
```typescript
// frontend/src/components/chat/ChatInterface.tsx
- Integrate with existing ChatInput component
- Sheet trigger on chat bar focus/click
- Responsive sizing (mobile vs desktop)
- Persistent across navigation
```

#### 1.4 Create Message Components
```typescript
// frontend/src/components/chat/MessageList.tsx
- Virtual scrolling for performance
- Auto-scroll to bottom on new messages
- Smooth animations

// frontend/src/components/chat/MessageBubble.tsx
- User vs assistant styling
- Markdown rendering
- Code syntax highlighting
- Timestamp display

// frontend/src/components/chat/ToolExecution.tsx
- Show tool name and status
- Loading spinner during execution
- Collapsible results
```

#### 1.5 Mode Selector
```typescript
// frontend/src/components/chat/ModeSelector.tsx
- Dropdown or button group
- Visual indicators (colors/icons)
- Tooltip explanations
```

**Potential Failure Points:**
- ‚ö†Ô∏è Sheet z-index conflicts with existing modals
- ‚ö†Ô∏è State persistence during navigation
- ‚ö†Ô∏è Mobile keyboard pushing content up
- ‚ö†Ô∏è Sheet not closing on route change

### Phase 2: API Integration (Day 2)

#### 2.1 Create Chat Service
```typescript
// frontend/src/services/chatService.ts
export class ChatService {
  private baseUrl = '/api/proxy/chat'
  
  async createConversation(mode: ConversationMode): Promise<Conversation>
  async listConversations(): Promise<Conversation[]>
  async deleteConversation(id: string): Promise<void>
  async getMessages(conversationId: string): Promise<Message[]>
}
```

#### 2.2 Authentication Integration (Updated)
```typescript
// Switch to HttpOnly cookie auth
// frontend/src/services/chatService.ts
export class ChatService {
  async loginWithCookies(email: string, password: string) {
    const response = await fetch('/api/proxy/auth/login', {
      method: 'POST',
      credentials: 'include', // Send cookies
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ email, password })
    })
    // Backend sets HttpOnly cookie automatically
    return response.json()
  }
}
```

#### 2.3 Conversation Lifecycle (Simplified)
```typescript
// On app mount:
1. Check cookie auth status (/auth/me)
2. Load existing conversations (cookies sent automatically)
3. Create conversation if needed
4. Load last 50 messages with backend pagination

// On message send:
1. Generate client run_id (UUID) for deduplication
2. Add user message to UI optimistically
3. POST to /chat/send with fetch() streaming
4. Parse streaming response and update UI
```

**Reduced Failure Points:**
- ‚úÖ No token expiration (session cookies)
- ‚úÖ Backend prevents duplicate conversations by user_id
- ‚úÖ Backend provides UUID conversation IDs
- ‚úÖ Backend has message pagination with cursor

### Phase 3: fetch() Streaming Implementation (Day 2-3)

#### 3.1 fetch() Streaming Hook
```typescript
// frontend/src/hooks/useFetchStreaming.ts
export function useFetchStreaming() {
  const streamChatMessage = async (conversationId: string, message: string, runId: string) => {
    const response = await fetch('/api/proxy/chat/send', {
      method: 'POST',
      credentials: 'include', // Cookies sent automatically
      headers: {
        'Content-Type': 'application/json',
        'Accept': 'text/event-stream'
      },
      body: JSON.stringify({ 
        conversation_id: conversationId, 
        text: message,
        run_id: runId // Client-generated for dedup
      })
    })
    
    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`)
    }
    
    const reader = response.body!.getReader()
    const decoder = new TextDecoder()
    
    try {
      while (true) {
        const { done, value } = await reader.read()
        if (done) break
        
        const chunk = decoder.decode(value, { stream: true })
        const events = parseSSEChunk(chunk)
        
        for (const event of events) {
          yield event
        }
      }
    } finally {
      reader.releaseLock()
    }
  }
}
```

#### 3.2 SSE Event Handling (Updated)
```typescript
// Event types from backend (same as before)
type SSEEventType = 
  | 'start'           // { conversation_id, mode, model, run_id }
  | 'message'         // { delta, role, run_id }
  | 'tool_started'    // { tool_name, arguments, run_id }
  | 'tool_finished'   // { tool_name, result, duration_ms, run_id }
  | 'done'           // { tool_calls_count, latency_ms, run_id }
  | 'error'          // { message, retryable, run_id }
  | 'heartbeat'      // { timestamp }

// Parser implementation for fetch() chunks
function parseSSEChunk(chunk: string): SSEEvent[] {
  const events: SSEEvent[] = []
  const lines = chunk.split('\n')
  let currentEvent = ''
  let currentData = ''
  
  for (const line of lines) {
    if (line.startsWith('event:')) {
      currentEvent = line.slice(6).trim()
    } else if (line.startsWith('data:')) {
      currentData = line.slice(5).trim()
    } else if (line === '' && currentEvent && currentData) {
      // Complete event found
      try {
        events.push({
          event: currentEvent,
          data: JSON.parse(currentData)
        })
      } catch (e) {
        console.warn('Failed to parse SSE event:', currentData)
      }
      // Reset for next event
      currentEvent = ''
      currentData = ''
    }
  }
  
  return events
}
```

#### 3.3 Stream Processing
```typescript
// Handle streaming text
const handleStreamingMessage = (delta: string) => {
  // Append to current message
  setStreamingMessage(prev => prev + delta)
  
  // Update UI immediately
  // Handle markdown parsing
  // Auto-scroll to bottom
}

// Handle tool execution
const handleToolExecution = (tool: ToolEvent) => {
  // Show tool indicator
  // Display loading state
  // Show results when complete
}
```

#### 3.4 Error Recovery
```typescript
// Reconnection with exponential backoff
const reconnectWithBackoff = async (
  attempt: number = 0,
  maxAttempts: number = 3
) => {
  const delay = Math.min(1000 * Math.pow(2, attempt), 10000)
  await new Promise(resolve => setTimeout(resolve, delay))
  
  try {
    await connectSSE()
  } catch (error) {
    if (attempt < maxAttempts) {
      return reconnectWithBackoff(attempt + 1, maxAttempts)
    }
    throw error
  }
}
```

**Potential Failure Points:**
- ‚ö†Ô∏è CORS blocking EventSource (need proxy config)
- ‚ö†Ô∏è SSE buffering (nginx/proxy buffering)
- ‚ö†Ô∏è Connection timeout (default 2 min limit)
- ‚ö†Ô∏è Message parsing errors (malformed JSON)
- ‚ö†Ô∏è Browser EventSource limitations (no headers)
- ‚ö†Ô∏è Network interruptions losing messages
- ‚ö†Ô∏è Memory leaks from unclosed connections

### Phase 4: Context Integration (Day 3)

#### 4.1 Portfolio Context
```typescript
// Inject current page context
const getPageContext = () => {
  const pathname = window.location.pathname
  const params = new URLSearchParams(window.location.search)
  
  if (pathname.includes('/portfolio')) {
    const portfolioType = params.get('type')
    return {
      page: 'portfolio',
      portfolioType,
      context: `User is viewing ${portfolioType} portfolio`
    }
  }
  // ... other pages
}

// Auto-inject context in first message
const enhancedMessage = `${userMessage}\n\n[Context: ${context}]`
```

#### 4.2 Smart Suggestions
```typescript
// Based on current page, suggest relevant queries
const getSuggestions = (page: string): string[] => {
  switch(page) {
    case 'portfolio':
      return [
        "What's my largest position?",
        "Show me my factor exposures",
        "Calculate my portfolio beta"
      ]
    case 'holdings':
      return [
        "Which stocks are down today?",
        "Show me sector allocation",
        "What's my cash balance?"
      ]
    // ...
  }
}
```

**Potential Failure Points:**
- ‚ö†Ô∏è Context injection revealing sensitive data
- ‚ö†Ô∏è Suggestions not updating on navigation
- ‚ö†Ô∏è Portfolio ID not available in context

### Phase 5: Mobile Optimization (Day 3-4)

#### 5.1 Responsive Behavior
```typescript
// Mobile: Full screen bottom sheet
// Tablet: Half screen side panel
// Desktop: 400px sidebar

const sheetSize = useMediaQuery({
  mobile: 'full',
  tablet: 'half',
  desktop: 'fixed'
})
```

#### 5.2 Touch Gestures
```typescript
// Swipe down to minimize
// Pull to refresh conversation
// Tap outside to close
```

#### 5.3 Keyboard Management
```typescript
// Adjust viewport when keyboard opens
// Scroll to input field
// Prevent content push
```

**Potential Failure Points:**
- ‚ö†Ô∏è iOS Safari viewport bugs
- ‚ö†Ô∏è Android keyboard overlapping input
- ‚ö†Ô∏è Sheet gesture conflicts with scroll
- ‚ö†Ô∏è Safe area insets (iPhone notch)

## Critical Failure Points & Mitigations (Updated for V1)

### 1. Authentication Failures ‚úÖ SOLVED
**Old Issue:** JWT token expires during conversation  
**V1 Solution:** HttpOnly session cookies eliminate token expiration issues
- Cookies managed by browser automatically
- No localStorage or token refresh needed
- Session length controlled by backend

### 2. Streaming Connection Issues ‚úÖ MOSTLY SOLVED
**Old Issue:** EventSource CORS/proxy complications  
**V1 Solution:** fetch() POST with credentials:'include'
```typescript
// Simplified proxy handling for fetch() streaming
const response = await fetch('/api/proxy/chat/send', {
  method: 'POST',
  credentials: 'include', // Cookies sent automatically
  headers: {
    'Content-Type': 'application/json',
    'Accept': 'text/event-stream'
  }
})
```
**Remaining concern:** Nginx buffering in production (deployment checklist)

### 3. Message Deduplication ‚úÖ SOLVED
**Old Issue:** Duplicate/overlapping message processing  
**V1 Solution:** Client-generated run_id + backend UUIDs
- Frontend generates UUID for each message send
- Backend conversation/message UUIDs prevent database duplicates  
- Stream events tagged with run_id for client-side buffering

### 4. Network Interruptions (Simplified Mitigation)
**Issue:** Lost messages during streaming  
**V1 Mitigation:**
- Simple retry with exponential backoff (3 attempts max)
- Show "connection lost" indicator
- No complex resume logic for V1 (full conversation reload on failure)

### 5. Memory Leaks ‚úÖ ADDRESSED
**Issue:** Unclosed streaming connections  
**V1 Solution:**
```typescript
// Cleanup with AbortController
const abortController = new AbortController()
const response = await fetch('/api/proxy/chat/send', {
  signal: abortController.signal,
  // ... other options
})

// Cleanup on unmount
useEffect(() => {
  return () => abortController?.abort()
}, [])
```

### 6. Rate Limiting
**Issue:** OpenAI rate limits  
**Mitigation:**
- Show clear error messages
- Implement client-side throttling
- Queue messages when rate limited

### 7. Large Responses
**Issue:** Tool results too large for UI  
**Mitigation:**
- Implement pagination
- Collapsible sections
- Virtual scrolling
- Truncate with "Show more"

## Testing Strategy

### Unit Tests
- Message parsing functions
- SSE event handling
- State management actions
- Context extraction

### Integration Tests
- Authentication flow
- Conversation creation
- Message sending
- SSE streaming
- Tool execution

### E2E Tests
- Complete chat flow
- Mode switching
- Mobile interactions
- Error recovery

### Manual Testing Checklist
- [ ] Chat bar opens Sheet
- [ ] Messages send and display
- [ ] Streaming text appears smoothly
- [ ] Tools show execution status
- [ ] Mode switching works
- [ ] Conversation persists on refresh
- [ ] Mobile gestures work
- [ ] Errors show user-friendly messages
- [ ] Network interruption recovery
- [ ] Long conversations scroll properly

## Performance Considerations

### Optimization Targets
- First message < 3s response
- Smooth streaming (60 fps)
- Sheet open < 200ms
- Memory usage < 50MB

### Optimization Techniques
- Virtual scrolling for long conversations
- Debounce streaming updates
- Lazy load conversation history
- Memoize expensive renders
- Use React.memo for message components

## Security Considerations

### Data Protection
- Sanitize markdown output
- No raw HTML rendering
- XSS prevention in user input
- Validate conversation ownership

### Token Management
- Secure token storage
- Auto-refresh before expiry
- Clear tokens on logout
- No tokens in URLs

## Rollout Strategy

### Phase 1: Internal Testing
- Deploy to staging
- Test with team members
- Gather feedback
- Fix critical bugs

### Phase 2: Beta Users
- Enable for subset of users
- Monitor error rates
- Track performance metrics
- Iterate on UX

### Phase 3: General Availability
- Full rollout
- Monitor closely
- Have rollback plan
- Document known issues

## Success Metrics

### Technical Metrics
- < 3s to first token
- < 10s full response
- < 1% error rate
- 99% uptime

### User Metrics
- > 50% engagement rate
- > 3 messages per session
- < 10% abandonment rate
- > 4.0 satisfaction score

## 7.0 Timeline (Updated for V1.1 Simplified Approach)

### 7.1 Week 1 - V1.1 Implementation
- **Day 1**: Switch authentication to HttpOnly cookies (modify auth endpoints)
- **Day 2**: Split store architecture (8.1) + Implement fetch() streaming hook with run_id deduplication
- **Day 3**: Connect existing ChatInterface UI to real backend endpoints  
- **Day 4**: Client-side message queue (8.2) + error handling improvements
- **Day 5**: Mobile input CSS fixes (8.3) + basic testing

### 7.2 Week 2 - Polish & Deploy  
- **Day 1-2**: Fix bugs + Observability/debugging system (8.5) with trace IDs
- **Day 3**: Create deployment checklist (8.4) for streaming infrastructure
- **Day 4**: Performance testing with real conversations
- **Day 5**: Deploy to staging, gather team feedback

**Key Simplifications for V1.1:**
- ‚úÖ No complex token refresh (cookies handle this)
- ‚úÖ No EventSource complications (fetch() streaming)
- ‚úÖ No custom pagination (use backend's limit=50)
- ‚úÖ Minimal reconnection logic (simple retry)
- ‚úÖ No virtual scrolling (fine for ~50 messages)

## 8.0 Additional Priority Features (From Feedback Review)

### 8.1 Split Store Architecture (Week 1, Day 2)
**Problem:** Mixing UI state and streaming state causes unnecessary re-renders  
**Solution:** Split Zustand stores into persistent vs runtime data

```typescript
// chatStore.ts - Persistent data only
interface ChatStore {
  conversations: Conversation[]
  messages: Record<string, Message[]>
  currentConversationId: string | null
  isSheetOpen: boolean
}

// streamStore.ts - Runtime state only  
interface StreamStore {
  isStreaming: boolean
  currentRunId: string | null
  streamBuffer: Map<string, string>
  abortController: AbortController | null
}
```

**Benefits:** Better performance, cleaner separation, easier debugging

### 8.2 Client-Side Message Queue (Week 1, Day 4)
**Problem:** Users can spam "send" button causing overlapping requests  
**Solution:** Implement message queue with visual feedback

```typescript
class MessageQueue {
  private queue: QueuedMessage[] = []
  private processing = false
  
  async add(message: string) {
    if (this.processing) {
      this.queue.push({ message, status: 'queued' })
      // Show "queued" badge in UI
      return
    }
    await this.sendMessage(message)
  }
}
```

**Benefits:** Prevents backend overload, better UX, clear user feedback

### 8.3 Mobile Input Handling (Week 1, Day 5)
**Problem:** Mobile keyboards cover input field, poor mobile UX  
**Solution:** CSS environment variables and scroll management

```css
.chat-input-container {
  padding-bottom: env(safe-area-inset-bottom);
  position: sticky;
  bottom: 0;
}

.chat-input:focus {
  scroll-margin-bottom: 20px;
}

/* iOS Safari specific fixes */
@supports (-webkit-touch-callout: none) {
  .chat-sheet {
    height: calc(100vh - env(safe-area-inset-bottom));
  }
}
```

**Benefits:** Mobile-first experience, keyboard doesn't block input, iPhone notch support

### 8.4 Deployment Checklist (Week 2, Day 3)
**Problem:** Infrastructure defaults break SSE streaming in production  
**Solution:** Comprehensive deployment checklist with nginx/proxy configs

**Create:** `deployment/STREAMING_CHECKLIST.md`

```nginx
# nginx.conf for streaming endpoints
location /api/v1/chat/send {
    proxy_buffering off;
    proxy_cache off;
    proxy_set_header Connection '';
    proxy_http_version 1.1;
    proxy_read_timeout 300s;
    proxy_send_timeout 300s;
}

# Cloudflare settings
- Disable "Rocket Loader" for streaming endpoints
- Set "Browser Cache TTL" to "Respect Existing Headers"
- Enable "Always Online" = OFF for real-time endpoints
```

**Additional Items:**
- Environment-specific CORS settings
- CDN/proxy buffer configurations  
- Timeout configurations (2+ minutes)
- Heartbeat intervals (‚â§15 seconds)
- Production monitoring setup

**Benefits:** Prevents production streaming failures, systematic deployment process

### 8.5 Observability & Debugging (Week 2, Day 1-2)
**Problem:** No way to debug when users report "it stopped working"  
**Solution:** Comprehensive trace IDs using existing backend ID system + structured logging

**Backend IDs Already Available:**
```typescript
// From backend agent system
interface TraceContext {
  conversation_id: string  // Backend conversation UUID
  message_id: string       // Backend message UUID  
  request_id: string       // Tool execution UUID (auto-generated)
  user_id: string         // User UUID
  run_id?: string         // Client-generated for dedup
}
```

**Frontend Implementation:**
```typescript
// Enhanced logging with trace context
class ChatLogger {
  static debug(event: string, context: TraceContext, data?: any) {
    const traceId = `${context.conversation_id.slice(0,8)}-${context.message_id?.slice(0,8) || 'none'}`
    
    console.debug(`[Chat:${event}]`, {
      traceId,
      timestamp: new Date().toISOString(),
      conversation_id: context.conversation_id,
      message_id: context.message_id,
      request_id: context.request_id,
      run_id: context.run_id,
      ...data
    })
  }
  
  static error(error: Error, context: TraceContext, retryable: boolean) {
    const errorEvent = {
      error: error.message,
      stack: error.stack,
      retryable,
      ...context
    }
    
    console.error('[Chat:Error]', errorEvent)
    
    // Send to monitoring in production
    if (process.env.NODE_ENV === 'production') {
      // analytics.track('chat_error', errorEvent)
    }
  }
}

// Usage in streaming hook
const streamMessage = async (text: string) => {
  const runId = uuidv4()
  const context = {
    conversation_id: currentConversation.id,
    message_id: null, // Will be set when message is created
    request_id: null, // Backend generates this
    user_id: currentUser.id,
    run_id: runId
  }
  
  ChatLogger.debug('stream_start', context, { text_length: text.length })
  
  try {
    // ... streaming logic
    ChatLogger.debug('stream_complete', context, { tokens: response.tokens })
  } catch (error) {
    ChatLogger.error(error, context, error.retryable || false)
  }
}
```

**Error Classification:**
```typescript
interface ErrorDetails {
  code: string
  retryable: boolean
  category: 'network' | 'auth' | 'server' | 'client' | 'rate_limit'
  context: TraceContext
}

// Distinguish error types for better debugging
const classifyError = (error: any): ErrorDetails => {
  if (error.status === 401) {
    return { code: 'AUTH_EXPIRED', retryable: false, category: 'auth' }
  }
  if (error.status === 429) {
    return { code: 'RATE_LIMITED', retryable: true, category: 'rate_limit' }
  }
  if (error.name === 'NetworkError') {
    return { code: 'NETWORK_ERROR', retryable: true, category: 'network' }
  }
  // ... more classifications
}
```

**Development Debugging:**
```typescript
// Add to chat store for debugging
interface DebugStore {
  lastError: ErrorDetails | null
  requestHistory: TraceContext[]
  streamingEvents: SSEEvent[]
  
  // Actions
  addTraceEvent: (context: TraceContext, event: string) => void
  getDebugInfo: () => DebugInfo
}

// Debug panel (development only)
const DebugPanel = () => (
  <div className="debug-panel">
    <h3>Chat Debug Info</h3>
    <pre>{JSON.stringify(useDebugStore().getDebugInfo(), null, 2)}</pre>
  </div>
)
```

**Benefits:** 
- Complete request traceability using existing backend UUIDs
- Distinguish retryable vs fatal errors  
- Development debugging tools
- Production error monitoring ready
- No new backend changes needed (leverages existing ID system)

## 9.0 V1 Code References (Updated)

### 9.1 Files Already Implemented ‚úÖ
- `/frontend/src/components/chat/ChatInterface.tsx` - Sheet UI complete
- `/frontend/src/stores/chatStore.ts` - State management done  
- `/frontend/src/app/components/ChatInput.tsx` - Input component ready

### 9.2 Files to Create üî®
- `/frontend/src/hooks/useFetchStreaming.ts` - fetch() streaming implementation
- `/frontend/src/services/chatService.ts` - Cookie-based API client  
- `/frontend/src/stores/streamStore.ts` - Separate streaming state store (8.1)
- `/frontend/src/hooks/useMessageQueue.ts` - Message queue management (8.2)
- `/frontend/src/styles/mobile-chat.css` - Mobile input handling styles (8.3)
- `/deployment/STREAMING_CHECKLIST.md` - Production deployment guide (8.4)
- `/frontend/src/utils/chatLogger.ts` - Observability with trace IDs (8.5)
- `/frontend/src/stores/debugStore.ts` - Development debugging tools (8.5)
- `/frontend/src/components/chat/MessageList.tsx` - Message display
- `/frontend/src/components/chat/MessageBubble.tsx` - Individual messages
- `/frontend/src/components/chat/ToolExecution.tsx` - Tool status
- `/frontend/src/components/chat/ModeSelector.tsx` - Mode switcher
- `/frontend/src/hooks/useSSEChat.ts` - SSE streaming hook
- `/frontend/src/services/chatService.ts` - API communication
- `/frontend/src/stores/chatStore.ts` - Zustand store
- `/frontend/src/types/chat.ts` - TypeScript interfaces

### Backend Endpoints (Already Implemented)
- `POST /api/v1/chat/conversations` - Create conversation
- `GET /api/v1/chat/conversations` - List conversations
- `DELETE /api/v1/chat/conversations/{id}` - Delete conversation
- `GET /api/v1/chat/conversations/{id}/messages` - Get message history
- `POST /api/v1/chat/send` - Send message (SSE response)

### Test Credentials
```typescript
const TEST_USER = {
  email: "demo_growth@sigmasight.com",
  password: "demo12345"
}
```

## Risk Register

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| SSE CORS issues | High | High | Use proxy, test early |
| Token expiration | Medium | Medium | Implement refresh flow |
| Mobile keyboard issues | High | Low | Test on real devices |
| Rate limiting | Low | High | Client-side throttling |
| Memory leaks | Medium | Medium | Proper cleanup hooks |
| Network interruptions | High | Low | Auto-reconnection |
| Large responses | Medium | Medium | Pagination/truncation |

## Conclusion

This implementation plan provides a comprehensive roadmap for integrating the chat interface with the existing backend. The Sheet overlay pattern preserves user context while providing a rich chat experience. Key success factors include proper SSE handling, robust error recovery, and careful attention to mobile UX.

The phased approach allows for iterative development with early validation of critical components. By identifying potential failure points upfront, we can build mitigations into the implementation rather than discovering issues in production.

Next steps:
1. Install shadcn Sheet component
2. Create basic Sheet UI structure  
3. Test with mock data
4. Integrate real API
5. Handle edge cases
6. Polish and optimize