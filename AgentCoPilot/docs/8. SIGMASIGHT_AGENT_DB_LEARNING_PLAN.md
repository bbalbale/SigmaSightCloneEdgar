# SigmaSight AI “Learning” – Database & Execution Plan

This document explains **how SigmaSight will support LLM “learning”** using the **existing PostgreSQL database**, plus a small set of AI‑specific tables and services.

It is meant for both human devs and AI coding agents working in `/backend`, `/frontend`, and `/AgentCoPilot`.

---

## 1. Design Goal

**We do _not_ create a new database.**  
All AI “learning” features live in the **existing Postgres** instance you already use for:

- portfolios, positions, snapshots, analytics,
- chat conversations and messages,
- AI insights and templates.

We only add **AI‑specific tables** and a **RAG (Retrieval Augmented Generation) helper**:

- No changes to core domain tables (unless explicitly requested).
- No extra external vector DB required (we use `pgvector` inside Postgres).

---

## 2. What “Learning” Means Here

The model weights do **not** change at runtime. “Learning” is implemented through:

1. **RAG / Knowledge Base**
   - Store docs, FAQs, tool docs, and curated answers.
   - Retrieve relevant chunks by semantic similarity and pass them into the prompt.

2. **Memories / Rules**
   - Persist user/tenant/global preferences and constraints, e.g.:
     - “Do not provide tax advice.”
     - “Default benchmark for this tenant is 60/40.”

3. **Logs & Feedback**
   - Log conversations, tool calls, and user feedback.
   - Use those logs offline to improve KB docs, prompts, and tools.

All of that is modeled as **data in Postgres**.

---

## 3. New AI Tables (In Existing Postgres)

We add **three new tables** (plus `pgvector` extension) via Alembic migration.

### 3.1 Enable `pgvector`

In the migration’s `upgrade()`:

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

We only enable this extension in the existing DB; no new DB or cluster is needed.

---

### 3.2 `ai_kb_documents` – RAG Knowledge Base

Stores documents and their embeddings for semantic search.

```sql
CREATE TABLE ai_kb_documents (
  id          uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  scope       text NOT NULL,    -- 'global', 'page:portfolio-overview', 'tenant:XYZ', etc.
  title       text NOT NULL,
  content     text NOT NULL,
  metadata    jsonb NOT NULL DEFAULT '{}'::jsonb,
  embedding   vector(1536),
  created_at  timestamptz NOT NULL DEFAULT now(),
  updated_at  timestamptz NOT NULL DEFAULT now()
);
```

Recommended indexes:

```sql
CREATE INDEX ix_ai_kb_documents_scope ON ai_kb_documents (scope);

CREATE INDEX ix_ai_kb_documents_embedding
ON ai_kb_documents
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

**Usage:**

- Populate with:
  - tool documentation (from `TOOL_REFERENCE.md`),
  - feature specs,
  - curated Q&A from real conversations.
- At query time:
  - Embed the user’s question.
  - Query by vector similarity.
  - Include the top K docs in the prompt.

---

### 3.3 `ai_memories` – Persistent Rules/Preferences

Stores cross‑cutting rules and preferences at global, tenant, or user scopes.

```sql
CREATE TABLE ai_memories (
  id          uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id     uuid,
  tenant_id   uuid,
  scope       text NOT NULL,        -- 'user' | 'tenant' | 'global'
  content     text NOT NULL,        -- short rule or fact
  tags        jsonb NOT NULL DEFAULT '{}'::jsonb,
  created_at  timestamptz NOT NULL DEFAULT now()
);
```

Indexes:

```sql
CREATE INDEX ix_ai_memories_scope ON ai_memories (scope);
CREATE INDEX ix_ai_memories_user_id ON ai_memories (user_id);
CREATE INDEX ix_ai_memories_tenant_id ON ai_memories (tenant_id);
```

**Usage:**

- Examples of `content`:
  - “Do not provide tax advice; always suggest consulting a tax professional.”
  - “Tenant ABC prefers volatility explained in plain language.”
- At runtime, you select relevant rows and inject them as “Rules and preferences” in the **system message** before calling the LLM.

---

### 3.4 `ai_feedback` – User Feedback on AI Messages

Connects user reactions to individual messages in `agent.MESSAGES` (existing table).

```sql
CREATE TABLE ai_feedback (
  id          uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  message_id  uuid NOT NULL,       -- references agent.MESSAGES.id
  rating      text NOT NULL,       -- 'up' | 'down'
  edited_text text,
  created_at  timestamptz NOT NULL DEFAULT now()
);
```

Index:

```sql
CREATE INDEX ix_ai_feedback_message_id ON ai_feedback (message_id);
```

**Usage:**

- Frontend exposes thumbs up/down on chat messages or insights.
- Backend writes to `ai_feedback` via a simple endpoint (e.g. `POST /api/v1/agent/messages/{id}/feedback`).
- Offline jobs or scripts:
  - Cluster questions by similarity.
  - Identify topics with poor feedback.
  - Suggest new KB docs or prompt/rule improvements.

---

## 4. Alembic Migration (Example)

Example migration file (outline) – see actual code in the repo for details.

```python
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from pgvector.sqlalchemy import Vector

revision = "2025xxxx_ai_rag"
down_revision = "<PREVIOUS_REVISION_ID>"

def upgrade() -> None:
    op.execute("CREATE EXTENSION IF NOT EXISTS vector")

    op.create_table(
        "ai_kb_documents",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text("gen_random_uuid()")),
        sa.Column("scope", sa.Text(), nullable=False),
        sa.Column("title", sa.Text(), nullable=False),
        sa.Column("content", sa.Text(), nullable=False),
        sa.Column("metadata", postgresql.JSONB(astext_type=sa.Text()), nullable=False, server_default=sa.text("'{}'::jsonb")),
        sa.Column("embedding", Vector(dim=1536), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False, server_default=sa.text("now()")),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=False, server_default=sa.text("now()")),
    )
    op.create_index("ix_ai_kb_documents_scope", "ai_kb_documents", ["scope"])
    op.execute(
        "CREATE INDEX IF NOT EXISTS ix_ai_kb_documents_embedding "
        "ON ai_kb_documents USING ivfflat (embedding vector_cosine_ops) "
        "WITH (lists = 100);"
    )

    op.create_table(
        "ai_memories",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text("gen_random_uuid()")),
        sa.Column("user_id", postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column("tenant_id", postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column("scope", sa.Text(), nullable=False),
        sa.Column("content", sa.Text(), nullable=False),
        sa.Column("tags", postgresql.JSONB(astext_type=sa.Text()), nullable=False, server_default=sa.text("'{}'::jsonb")),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False, server_default=sa.text("now()")),
    )
    op.create_index("ix_ai_memories_scope", "ai_memories", ["scope"])
    op.create_index("ix_ai_memories_user_id", "ai_memories", ["user_id"])
    op.create_index("ix_ai_memories_tenant_id", "ai_memories", ["tenant_id"])

    op.create_table(
        "ai_feedback",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text("gen_random_uuid()")),
        sa.Column("message_id", postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column("rating", sa.Text(), nullable=False),
        sa.Column("edited_text", sa.Text(), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False, server_default=sa.text("now()")),
    )
    op.create_index("ix_ai_feedback_message_id", "ai_feedback", ["message_id"])


def downgrade() -> None:
    op.drop_index("ix_ai_feedback_message_id", table_name="ai_feedback")
    op.drop_table("ai_feedback")

    op.drop_index("ix_ai_memories_tenant_id", table_name="ai_memories")
    op.drop_index("ix_ai_memories_user_id", table_name="ai_memories")
    op.drop_index("ix_ai_memories_scope", table_name="ai_memories")
    op.drop_table("ai_memories")

    op.drop_index("ix_ai_kb_documents_scope", table_name="ai_kb_documents")
    op.execute("DROP INDEX IF EXISTS ix_ai_kb_documents_embedding")
    op.drop_table("ai_kb_documents")
```

> Note: we usually do **not** drop the `vector` extension in `downgrade()` because other tables may still depend on it.

---

## 5. RAG Service – `rag_service.py`

To keep the AI logic simple, we create a small helper module that:

- Uses OpenAI embeddings to embed text.
- Inserts documents into `ai_kb_documents`.
- Retrieves the most relevant docs for a given query.

**File:** `backend/app/agent/services/rag_service.py`

```python
from __future__ import annotations

from typing import Any, Dict, List, Optional, Sequence

import os
from openai import AsyncOpenAI
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
from app.core.logging import get_logger

logger = get_logger(__name__)

EMBEDDING_MODEL = "text-embedding-3-small"

_client: Optional[AsyncOpenAI] = None


def get_openai_client() -> AsyncOpenAI:
    global _client
    if _client is None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY is not set for RAG embeddings")
        _client = AsyncOpenAI(api_key=api_key)
    return _client


async def embed_text(text_value: str) -> List[float]:
    client = get_openai_client()
    resp = await client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=text_value,
    )
    return resp.data[0].embedding  # type: ignore[no-any-return]


async def upsert_kb_document(
    db: AsyncSession,
    *,
    scope: str,
    title: str,
    content: str,
    metadata: Optional[Dict[str, Any]] = None,
) -> None:
    metadata = metadata or {}
    embedding = await embed_text(content)

    stmt = text(
        """
        INSERT INTO ai_kb_documents (scope, title, content, metadata, embedding)
        VALUES (:scope, :title, :content, :metadata, :embedding)
        """
    )

    await db.execute(
        stmt,
        {
            "scope": scope,
            "title": title,
            "content": content,
            "metadata": metadata,
            "embedding": embedding,
        },
    )
    await db.commit()

    logger.info(f"[RAG] Inserted KB doc scope={scope!r} title={title!r}")


async def retrieve_relevant_docs(
    db: AsyncSession,
    *,
    query: str,
    scopes: Optional[Sequence[str]] = None,
    limit: int = 5,
) -> List[Dict[str, Any]]:
    embedding = await embed_text(query)

    if scopes:
        stmt = text(
            """
            SELECT id, scope, title, content, metadata
            FROM ai_kb_documents
            WHERE scope = ANY(:scopes)
            ORDER BY embedding <-> :embedding
            LIMIT :limit
            """
        )
        params: Dict[str, Any] = {
            "scopes": list(scopes),
            "embedding": embedding,
            "limit": limit,
        }
    else:
        stmt = text(
            """
            SELECT id, scope, title, content, metadata
            FROM ai_kb_documents
            ORDER BY embedding <-> :embedding
            LIMIT :limit
            """
        )
        params = {
            "embedding": embedding,
            "limit": limit,
        }

    result = await db.execute(stmt, params)
    rows = result.mappings().all()

    docs: List[Dict[str, Any]] = []
    for row in rows:
        docs.append(
            {
                "id": str(row["id"]),
                "scope": row["scope"],
                "title": row["title"],
                "content": row["content"],
                "metadata": row["metadata"],
            }
        )

    logger.info(f"[RAG] Retrieved {len(docs)} docs for query={query!r}, scopes={scopes}")
    return docs
```

This module is deliberately minimal and uses **raw SQL + `AsyncSession`** so it slots easily into your existing backend without new ORM models.

---

## 6. Wiring RAG Into the Agent Flow

Here’s where RAG hooks into `/chat/send` and insights.

### 6.1 Determining Scopes

When a request comes into your SSE generator / OpenAI service, you can derive scopes like:

```python
scopes = ["global"]

# Page / feature scope
scopes.append("page:sigmasight-ai")  # or page:portfolio-overview, etc.

# Tenant / portfolio specific scope
if portfolio_context and portfolio_context.get("portfolio_id"):
    scopes.append(f"portfolio:{portfolio_context['portfolio_id']}")
```

### 6.2 Retrieving Docs Per Request

Before calling the LLM (inside `openai_service` or a new `llm_openai` client), call:

```python
from app.agent.services import rag_service

kb_docs = await rag_service.retrieve_relevant_docs(
    db,
    query=message_text,  # or richer query including context
    scopes=scopes,
    limit=3,
)
```

Then include `kb_docs` (as formatted text snippets) in the **system/assistant messages** you pass to the OpenAI Responses API.

Example prompt pattern:

```text
You are SigmaSight's portfolio risk assistant.

Context documents (may or may not be relevant):
[1] {title_1}
{content_1}

[2] {title_2}
{content_2}

User question:
{message_text}
```

The rest of your SSE flow, tools, and conversation logging stay exactly as they are.

---

## 7. Execution Plan for the DB + RAG Layer

1. **Add migration** for:
   - `CREATE EXTENSION IF NOT EXISTS vector;`
   - `ai_kb_documents`
   - `ai_memories`
   - `ai_feedback`

2. **Implement `rag_service.py`** as shown above.

3. **Seed initial KB docs** (optional but recommended):
   - `TOOL_REFERENCE.md` (tool descriptions, inputs, outputs).
   - High‑level SigmaSight architecture notes.
   - Portfolio risk methodology primers.

4. **Wire RAG into chat/insights**:
   - In your agent OpenAI path (currently in `openai_service.stream_responses` or a new `llm_openai` client), call `retrieve_relevant_docs` and inject snippets into prompts.

5. **Add feedback endpoint & UI** (Phase 3):
   - Endpoint: e.g. `POST /api/v1/agent/messages/{id}/feedback` → insert into `ai_feedback`.
   - Frontend: thumbs up/down on chat/insights, posting to that endpoint.

6. **Offline learning loop** (later phase):
   - Script or admin UI to:
     - Read `agent.MESSAGES`, `AI_INSIGHTS`, `ai_feedback`,
     - Cluster questions with low ratings,
     - Propose new KB docs or rule updates,
     - Insert or update records in `ai_kb_documents` and `ai_memories`.

This keeps all AI learning behavior **inside your existing Postgres**, with **minimal new schema**, and aligns with the rest of the SigmaSight architecture.
