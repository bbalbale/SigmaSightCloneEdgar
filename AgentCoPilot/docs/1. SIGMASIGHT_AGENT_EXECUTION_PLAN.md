# SigmaSight Agent â€“ Execution Plan & Provider Strategy

This document captures the **execution plan** for SigmaSight's AI agent, including:
- Model/provider choices (OpenAI vs Claude)
- Target architecture on Railway
- Step-by-step rollout phases
- Concrete TODOs for `/backend` and `/frontend`

It is meant to guide **both human developers and AI coding agents**.

**Last Updated:** 2025-12-14

---

## Current Implementation Status

### Completed
- [x] **Phase 0** â€“ Config & Folder Setup (COMPLETE)
- [x] **Phase 1** â€“ Clean Provider Abstraction (COMPLETE)
- [x] **Phase 2** â€“ RAG Layer Foundation (COMPLETE - Dec 14, 2025)
  - DB schema: `ai_kb_documents`, `ai_memories`, `ai_feedback` tables with pgvector
  - RAG service: `rag_service.py` with embedding + vector similarity search
  - KB seeding: 50 documents (22 tool docs, 17 domain primers, 11 FAQs)
  - Integration: RAG context injected into `/chat/send` system prompt
- [x] **Phase 3** â€“ Feedback Endpoint (COMPLETE - backend + frontend)
- [x] **Copilot UI Refactor** â€“ Reusable components (COMPLETE)

### In Progress
- [ ] **Phase 3** â€“ Offline feedback analysis scripts
- [ ] **Phase 4** â€“ Multi-Provider & Advanced Routing (planning)

### Not Started
- [ ] **Phase 5** â€“ Advanced RAG (user memories, conversation context)

---

## 1. Provider Strategy â€“ OpenAI First, Claude-Ready

### 1.1 Default: OpenAI

For SigmaSight's current stack (Python backend, Next.js frontend, Railway hosting), we use **OpenAI as the default provider** because:

- Excellent Python + JS SDK support
- First-class support for:
  - Responses API (streaming + tools + RAG)
  - Large context windows
- Good price-performance for both:
  - **fast, cheap models** (e.g. `gpt-4o-mini` for routine tasks)
  - **strong reasoning models** (e.g. `gpt-4o` for complex analyses)

We follow this pattern:

- **Default model** for most agent calls: `gpt-4o-mini`
- **"Heavy" model** for complex tasks: `gpt-4o` (called selectively)

### 1.2 Optional: Claude (Anthropic)

Claude models (e.g. Claude Sonnet/Opus) are excellent for some coding and agentic tasks, but are typically more expensive and not strictly required as you start.

Plan:

- Design a **provider abstraction** so we can plug in Claude later without rewriting endpoints.
- Start with only OpenAI wired through that abstraction.
- Later, optionally route some tasks to Claude (e.g. long-form research or advanced chain-of-thought) while keeping OpenAI as the default.

### 1.3 LLM Client Abstraction (Backend) â€“ IMPLEMENTED

**Location:** `/backend/app/agent/llm_client.py`

The provider-agnostic interface is now implemented:

```python
# backend/app/agent/llm_client.py

from typing import Protocol, AsyncIterator, Dict, List, Any
from enum import Enum

class LLMProvider(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"

class LLMClient(Protocol):
    @property
    def provider(self) -> LLMProvider:
        ...

    async def stream_response(
        self,
        messages: List[Dict[str, Any]],
        tools: List[Dict[str, Any]] | None = None,
        model: str | None = None,
        system_prompt: str | None = None,
        temperature: float = 0.7,
        max_tokens: int | None = None,
    ) -> AsyncIterator[str]:
        ...

def get_llm_client(provider: LLMProvider | None = None) -> LLMClient:
    """Factory function to get an LLM client.

    Currently defaults to OpenAI. Can be extended to support
    multiple providers via environment variables or feature flags.
    """
    ...
```

**OpenAI Implementation:** `/backend/app/agent/llm_openai.py`

The `OpenAILLMClient` wraps the existing `OpenAIService` and conforms to the `LLMClient` protocol.


---

## 2. Target Architecture (Railway + Existing Backend/Frontend)

We keep the current structure and add **only what's needed**:

```text
repo-root/
  backend/        # FastAPI app, all APIs, agent runtime, tools
  frontend/       # Next.js 14 app, all UI
  AgentCoPilot/   # AI coding agent docs + config (THIS folder)
```

On Railway, you likely have or will have:

- Service: **backend** (FastAPI)
  - Exposes: `/api/v1/...` including `/chat/*`, `/insights/*`, `/analytics/*`, etc.
  - Talks to: Postgres + any external data providers.

- Service: **web** (Next.js)
  - Renders all pages under `/frontend`.
  - Proxies API calls to the backend (`/api/proxy/[...path]` pattern).

- Service: **db** (Postgres + pgvector)
  - Stores all core tables (`PORTFOLIOS`, `POSITIONS`, `PORTFOLIO_SNAPSHOTS`, etc.).
  - Also stores AI infra tables (`ai_kb_documents`, `ai_memories`, `ai_feedback`) - **SCHEMA CREATED**.

We **do not** create a separate "ai-api" repo/service right now; instead we:

- Implement the agent runtime inside `/backend/app/agent/`
- Expose behavior through existing endpoints:
  - `/api/v1/chat/send` (SSE) - **WORKING**
  - `/api/v1/chat/conversations` (conversation management) - **WORKING**
  - `/api/v1/agent/messages/{id}/feedback` (feedback collection) - **IMPLEMENTED**
- Optionally add new endpoints under `/api/v1/agent/*` later if needed.


---

## 3. Make It Fast: Streaming + Right Model Choice â€“ IMPLEMENTED

### Current State

- SSE streaming from FastAPI (`/chat/send`) to the frontend - **WORKING**
- Provider-agnostic naming in frontend:
  - `aiChatStore.ts` (renamed from `claudeInsightsStore.ts`)
  - `aiChatService.ts` (renamed from `claudeInsightsService.ts`)
  - `AIChatInterface.tsx` (renamed from `ClaudeChatInterface.tsx`)
- Reusable Copilot components:
  - `useCopilot.ts` hook
  - `CopilotPanel.tsx` component
  - `CopilotButton.tsx` floating action button
  - `CopilotSheet.tsx` slide-out panel

### SSE Event Flow

The backend emits these events:
- `start` - Stream started, includes run_id
- `message_created` - Backend message ID for feedback tracking
- `token` - Text delta from OpenAI Responses API
- `tool_call` - Tool invocation started
- `tool_result` - Tool completed with result
- `done` - Stream complete with final_text and tool_calls_count
- `error` - Error occurred

### Model Selection

Currently using OpenAI models via the Responses API:
- Default: `gpt-4o-mini` for most chat turns
- Can switch to `gpt-4o` for complex analysis (configured in `openai_service.py`)


---

## 4. Make It "Learn": RAG + Logs + Feedback â€“ PARTIAL

### 4.1 RAG with pgvector (in existing Postgres) â€“ SCHEMA CREATED

Database migration created: `l9m0n1o2p3q4_add_ai_learning_tables.py`

```sql
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE ai_kb_documents (
  id          uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  scope       text NOT NULL,   -- 'global', 'page:portfolio-overview', 'tenant:XYZ', etc.
  title       text NOT NULL,
  content     text NOT NULL,
  metadata    jsonb NOT NULL DEFAULT '{}'::jsonb,
  embedding   vector(1536),    -- match the embedding model used
  created_at  timestamptz NOT NULL DEFAULT now(),
  updated_at  timestamptz NOT NULL DEFAULT now()
);

CREATE TABLE ai_memories (
  id          uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id     uuid,
  tenant_id   uuid,
  scope       text NOT NULL,   -- 'user' | 'tenant' | 'global'
  content     text NOT NULL,
  tags        jsonb DEFAULT '{}',
  created_at  timestamptz NOT NULL DEFAULT now()
);

CREATE TABLE ai_feedback (
  id          uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  message_id  uuid NOT NULL,   -- references agent.MESSAGES.id
  rating      text NOT NULL,   -- 'up' | 'down'
  edited_text text,
  created_at  timestamptz NOT NULL DEFAULT now()
);
```

**RAG Service:** `/backend/app/agent/services/rag_service.py` - IMPLEMENTED

```python
class RAGService:
    async def embed_text(self, text: str) -> List[float]:
        """Generate embedding using OpenAI text-embedding-3-small"""
        ...

    async def upsert_kb_document(
        self, scope: str, title: str, content: str, metadata: dict = None
    ) -> str:
        """Add or update a document with its embedding"""
        ...

    async def retrieve_relevant_docs(
        self, query: str, scopes: List[str], limit: int = 5
    ) -> List[dict]:
        """Vector search for relevant documents"""
        ...
```

**Next Steps:**
- [ ] Seed KB with tool documentation (`TOOL_REFERENCE.md`)
- [ ] Add domain primers (volatility, beta, concentration, etc.)
- [ ] Wire RAG into `/chat/send` before LLM calls

### 4.2 Interaction Logging & Feedback â€“ IMPLEMENTED

**Backend:**
- Endpoint: `POST /api/v1/agent/messages/{message_id}/feedback`
- Location: `/backend/app/api/v1/chat/feedback.py`
- Supports rating ('up' | 'down') and optional edited_text

**Frontend:**
- Feedback service: `/frontend/src/services/feedbackService.ts`
- Thumbs up/down buttons on assistant messages
- Optimistic UI updates with error rollback

**Flow:**
1. Assistant message includes `backendMessageId` from `message_created` event
2. User clicks thumbs up/down
3. Frontend calls `submitFeedback(backendMessageId, { rating })`
4. Backend stores in `ai_feedback` table


---

## 5. Phase-by-Phase Execution Plan

### Phase 0 â€“ Config & Folder Setup âœ… COMPLETE

**Goal:** Prepare the repo so humans + AI agents can work consistently.

- [x] Create `/AgentCoPilot` with all documentation files
- [x] Add API/DB summary as `SIGMASIGHT_API_DB_SUMMARY.md`
- [x] Numbered documentation files for organization

### Phase 1 â€“ Clean Provider Abstraction âœ… COMPLETE

**Goal:** Have a single, clean code path for LLM calls using OpenAI.

Backend:
- [x] Created `/backend/app/agent/llm_client.py` with `LLMClient` protocol
- [x] Created `/backend/app/agent/llm_openai.py` with `OpenAILLMClient`
- [x] `/chat/send` uses OpenAI Responses API with SSE streaming
- [x] 18 tools registered for portfolio data access

Frontend:
- [x] Renamed `claudeInsightsStore.ts` â†’ `aiChatStore.ts`
- [x] Renamed `claudeInsightsService.ts` â†’ `aiChatService.ts`
- [x] Renamed `claude-insights/` â†’ `ai-chat/`
- [x] Renamed `ClaudeChatInterface` â†’ `AIChatInterface`
- [x] Created `useCopilot` hook for reusable chat functionality
- [x] Created `CopilotPanel`, `CopilotButton`, `CopilotSheet` components
- [x] Removed all backward compatibility aliases

### Phase 2 â€“ RAG Layer â³ PARTIAL

**Goal:** Allow the agent to use an internal knowledge base instead of ever-longer prompts.

Backend:
- [x] Added `ai_kb_documents`, `ai_memories` tables via Alembic migration
- [x] Implemented `RAGService` in `/backend/app/agent/services/rag_service.py`
- [ ] Seed KB with tool documentation
- [ ] Seed KB with domain primers
- [ ] Wire RAG into `/chat/send` flow

Admin endpoints (optional):
- [ ] `POST /api/v1/agent/kb/index` â€“ bulk ingest docs
- [ ] `GET /api/v1/agent/kb/search` â€“ preview RAG results

### Phase 3 â€“ Logging, Feedback, and Learning â³ PARTIAL

**Goal:** Use real user interactions to improve behavior over time.

Backend:
- [x] `agent.CONVERSATIONS` and `agent.MESSAGES` populated with all needed fields
- [x] Added `ai_feedback` table
- [x] Implemented `POST /api/v1/agent/messages/{id}/feedback` endpoint
- [ ] Create feedback analysis script

Frontend:
- [x] Added feedback UI (thumbs up/down) to chat messages
- [x] Wired controls to feedback endpoint
- [ ] Optional: "Edit response" feature for detailed feedback

### Phase 4 â€“ Multi-Provider & Advanced Routing ğŸ”² NOT STARTED

**Goal:** Be able to plug in Claude or other providers easily.

Backend TODOs:
- [ ] Implement `/backend/app/agent/llm_claude.py` for Anthropic
- [ ] Update `get_llm_client()` to choose provider based on config
- [ ] Add routing logic for heavy vs light tasks

Frontend:
- No changes needed; talks to `/chat/send` via SSE regardless of provider


---

## 6. File Locations Reference

### Backend Files

```
backend/app/agent/
â”œâ”€â”€ llm_client.py              # LLMClient protocol & get_llm_client()
â”œâ”€â”€ llm_openai.py              # OpenAILLMClient implementation
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ openai_service.py      # OpenAI Responses API service (18 tools)
â”‚   â””â”€â”€ rag_service.py         # RAG service for knowledge base
â””â”€â”€ tools/                     # Tool implementations

backend/app/api/v1/chat/
â”œâ”€â”€ router.py                  # Chat router
â”œâ”€â”€ send.py                    # /chat/send SSE endpoint
â””â”€â”€ feedback.py                # /agent/messages/{id}/feedback endpoint

backend/app/models/
â””â”€â”€ ai_learning.py             # ORM models for ai_kb_documents, ai_memories, ai_feedback

backend/alembic/versions/
â””â”€â”€ l9m0n1o2p3q4_add_ai_learning_tables.py  # Migration for AI tables
```

### Frontend Files

```
frontend/src/
â”œâ”€â”€ stores/
â”‚   â””â”€â”€ aiChatStore.ts         # Zustand store for AI chat state
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ aiChatService.ts       # SSE streaming service
â”‚   â””â”€â”€ feedbackService.ts     # Feedback submission service
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ useCopilot.ts          # Reusable copilot hook
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ai-chat/
â”‚   â”‚   â””â”€â”€ AIChatInterface.tsx  # Standalone chat interface
â”‚   â””â”€â”€ copilot/
â”‚       â”œâ”€â”€ CopilotPanel.tsx     # Reusable chat panel
â”‚       â”œâ”€â”€ CopilotButton.tsx    # Floating action button
â”‚       â”œâ”€â”€ CopilotSheet.tsx     # Slide-out sheet
â”‚       â””â”€â”€ index.ts             # Exports
â””â”€â”€ containers/
    â””â”€â”€ SigmaSightAIContainer.tsx  # Uses CopilotPanel
```


---

## 7. How This Ties Back To The Other AgentCoPilot Docs

- **`0. SIGMASIGHT_AGENT_OVERVIEW.md`** â€“ Roles, repo layout, global rules.
- **`0.i. SIGMASIGHT_AGENT_WORKFLOW.md`** â€“ Day-to-day workflow and quality checklist.
- **`1. SIGMASIGHT_AGENT_EXECUTION_PLAN.md`** (this file) â€“ Concrete execution roadmap and provider strategy.
- **`2. SIGMASIGHT_AGENT_BACKEND.md`** â€“ Detailed backend rules, endpoints, and tools.
- **`3. SIGMASIGHT_AGENT_FRONTEND.md`** â€“ Frontend integration, SSE, and UI context.
- **`4. SIGMASIGHT_AGENT_TOOLS_AND_DATA.md`** â€“ Data contracts and anti-hallucination rules.
- **`5. SIGMASIGHT_AGENT_CONTEXT_AND_LEARNING.md`** â€“ RAG, memories, feedback schemas.
- **`6. SIGMASIGHT_AGENT_DBUPGRADE.md`** â€“ Database migration details.
- **`7. SIGMASIGHT_API_DB_SUMMARY.md`** â€“ API endpoint reference.
- **`8. SIGMASIGHT_AGENT_DB_LEARNING_PLAN.md`** â€“ Learning engine implementation plan.

AI coding agents should read all of these, but use this execution plan as the **"what do I build next?"** guide.
