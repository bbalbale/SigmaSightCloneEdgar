# SigmaSight Agent – Execution Plan & Provider Strategy

This document captures the **execution plan** for SigmaSight’s AI agent, including:
- Model/provider choices (OpenAI vs Claude)
- Target architecture on Railway
- Step‑by‑step rollout phases
- Concrete TODOs for `/backend` and `/frontend`

It is meant to guide **both human developers and AI coding agents**.


---

## 1. Provider Strategy – OpenAI First, Claude‑Ready

### 1.1 Default: OpenAI

For SigmaSight’s current stack (Python backend, Next.js frontend, Railway hosting), we use **OpenAI as the default provider** because:

- Excellent Python + JS SDK support
- First‑class support for:
  - Responses API (streaming + tools + RAG)
  - Large context windows
- Good price‑performance for both:
  - **fast, cheap models** (e.g. `gpt-5-mini` for routine tasks)
  - **strong reasoning models** (e.g. `gpt-5.1` for complex analyses)

We follow this pattern:

- **Default model** for most agent calls: `gpt-5-mini`
- **“Heavy” model** for complex tasks: `gpt-5.1` (called selectively)

### 1.2 Optional: Claude (Anthropic)

Claude models (e.g. Claude Sonnet/Opus) are excellent for some coding and agentic tasks, but are typically more expensive and not strictly required as you start.

Plan:

- Design a **provider abstraction** so we can plug in Claude later without rewriting endpoints.
- Start with only OpenAI wired through that abstraction.
- Later, optionally route some tasks to Claude (e.g. long‑form research or advanced chain‑of‑thought) while keeping OpenAI as the default.

### 1.3 LLM Client Abstraction (Backend)

In `/backend/app/agent/`, introduce a simple provider‑agnostic interface, for example:

- `llm_client.py` – main interface used by the rest of the backend
- `llm_openai.py` – OpenAI implementation
- `llm_claude.py` – (optional) Anthropic implementation

Example interface sketch:

```python
# backend/app/agent/llm_client.py

from typing import AsyncIterator, Protocol, Any, Dict, List

class LLMClient(Protocol):
    async def stream_response(
        self,
        messages: List[Dict[str, Any]],
        tools: List[Dict[str, Any]] | None = None,
        model: str | None = None,
        metadata: Dict[str, Any] | None = None,
    ) -> AsyncIterator[Dict[str, Any]]:
        """Yield structured events compatible with our SSE layer.

        Each yielded dict should already be in the form our SSE endpoint expects,
        e.g.: {"type": "text_delta", "delta": "..."} or {"type": "tool_call", ...}
        """
        ...

# Somewhere central:
def get_llm_client() -> LLMClient:
    # For now, always return OpenAI implementation.
    # Later, this can branch on env var / conversation.mode / feature flag.
    ...
```

The existing `/chat/send` and `/insights/generate` code should call `get_llm_client().stream_response(...)` instead of directly calling OpenAI.


---

## 2. Target Architecture (Railway + Existing Backend/Frontend)

We keep the current structure and add **only what’s needed**:

```text
repo-root/
  backend/        # FastAPI app, all APIs, agent runtime, tools
  frontend/       # Next.js 14 app, all UI
  AgentCoPilot/   # AI coding agent docs + config (THIS folder)
```

On Railway, you likely have or will have:

- Service: **backend** (FastAPI)  
  - Exposes: `/api/v1/...` including `/chat/*`, `/insights/*`, `/analytics/*`, etc.
  - Talks to: Postgres + any external data providers.

- Service: **web** (Next.js)  
  - Renders all pages under `/frontend`.
  - Proxies API calls to the backend (`/api/proxy/[...path]` pattern).

- Service: **db** (Postgres + pgvector)  
  - Stores all core tables (`PORTFOLIOS`, `POSITIONS`, `PORTFOLIO_SNAPSHOTS`, etc.).
  - Also stores AI infra tables (`ai_kb_documents`, `ai_memories`, etc.) when added.

We **do not** create a separate “ai-api” repo/service right now; instead we:

- Implement the agent runtime inside `/backend/app/agent/`
- Expose behavior through existing endpoints:
  - `/api/v1/chat/send` (SSE)
  - `/api/v1/insights/generate` (existing insights engine)
- Optionally add new endpoints under `/api/v1/agent/*` later if needed.


---

## 3. Make It Fast: Streaming + Right Model Choice

You already have:

- SSE streaming from FastAPI (`/chat/send`) to the frontend
- A `ClaudeChatInterface` + Zustand store + `claudeInsightsService` that handles SSE events

Execution plan:

1. **Keep the SSE plumbing as is.**  
   - The backend still emits `start`, `message`, `tool_call`, `tool_result`, `done`, `error` events.
   - The frontend continues to use the same SSE parsing logic.

2. **Put the Responses API behind `llm_client`.**  
   - Instead of calling OpenAI directly inside `/chat/send`, call `LLMClient.stream_response(...)`.
   - Inside `llm_openai.py`, use OpenAI Responses API to:
     - stream text,
     - handle tool calls,
     - map responses into the event shapes your SSE layer expects.

3. **Use `gpt-5-mini` as default.**  
   - For most chat turns, the backend uses `gpt-5-mini` via `LLMClient`.
   - For expensive / heavy analysis (e.g. “do a full 30‑day portfolio post‑mortem”), the backend can explicitly request `gpt-5.1`.


---

## 4. Make It “Learn”: RAG + Logs + Feedback

We’ll layer learning on top of your existing DB, tools, and APIs.

### 4.1 RAG with pgvector (in existing Postgres)

Add AI‑specific tables (example):

```sql
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE ai_kb_documents (
  id          uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  scope       text NOT NULL,   -- 'global', 'page:portfolio-overview', 'tenant:XYZ', etc.
  title       text NOT NULL,
  content     text NOT NULL,
  metadata    jsonb NOT NULL DEFAULT '{}'::jsonb,
  embedding   vector(1536),    -- match the embedding model used
  created_at  timestamptz NOT NULL DEFAULT now(),
  updated_at  timestamptz NOT NULL DEFAULT now()
);
```

Flow for `/chat/send` / `/insights/generate`:

1. Embed the user’s question (and optionally current portfolio context).  
2. Query `ai_kb_documents` by vector similarity to find top K relevant docs.  
3. Build prompt with:
   - system rules (SigmaSight persona + page context),
   - retrieved docs,
   - user question.  
4. Call `LLMClient.stream_response(...)` with that context.

You populate `ai_kb_documents` with:

- Tool docs (`TOOL_REFERENCE.md`)
- Domain write‑ups (volatility, beta, stress, concentration, etc.)
- Curated “good answers” from real conversations.

### 4.2 Interaction Logging & Feedback

You already have:

- `agent.CONVERSATIONS`
- `agent.MESSAGES`
- `AI_INSIGHTS` and `AI_INSIGHT_TEMPLATES`

Add an AI‑specific feedback table if/when needed:

```sql
CREATE TABLE ai_feedback (
  id          uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  message_id  uuid NOT NULL,      -- references agent.MESSAGES.id
  rating      text NOT NULL,      -- 'up' | 'down'
  edited_text text,
  created_at  timestamptz NOT NULL DEFAULT now()
);
```

Frontend work:

- Add thumbs up/down + optional “edit answer” for chat messages and/or insights.
- POST to a new backend endpoint that writes to `ai_feedback`.

Offline analysis (script or admin UI) will:

- Cluster questions via embeddings on `agent.MESSAGES`.
- Identify:
  - high‑volume topics,
  - topics with poor feedback,
  - tool call patterns (e.g. missing/incorrect tools).

Human or AI curation then:

- Improves docs in `ai_kb_documents`,
- Tweaks prompts/rules,
- Suggests new tools when needed.


---

## 5. Phase‑by‑Phase Execution Plan

### Phase 0 – Config & Folder Setup

**Goal:** Prepare the repo so humans + AI agents can work consistently.

- [x] Create `/AgentCoPilot` (DONE) with:
  - `SIGMASIGHT_AGENT_OVERVIEW.md`
  - `SIGMASIGHT_AGENT_BACKEND.md`
  - `SIGMASIGHT_AGENT_FRONTEND.md`
  - `SIGMASIGHT_AGENT_TOOLS_AND_DATA.md`
  - `SIGMASIGHT_AGENT_CONTEXT_AND_LEARNING.md`
  - `SIGMASIGHT_AGENT_WORKFLOW.md`
  - `SIGMASIGHT_AGENT_EXECUTION_PLAN.md` (this file)
- [ ] Add your API/DB summary as `SIGMASIGHT_API_DB_SUMMARY.md`.
- [ ] Make sure Cursor/Windsurf/etc. include `/AgentCoPilot` as high‑priority context.

### Phase 1 – Clean Provider Abstraction (OpenAI only)

**Goal:** Have a single, clean code path for LLM calls using OpenAI.

Backend TODOs:

1. Create `/backend/app/agent/llm_openai.py` implementing OpenAI Responses API calls.
2. Create `/backend/app/agent/llm_client.py` defining `LLMClient` and `get_llm_client()`.
3. Refactor `/backend/app/api/v1/chat.py` (or wherever `/chat/send` lives) to:
   - Build messages (system + history + user + tools),
   - Call `get_llm_client().stream_response(...)`,
   - Stream events to frontend via SSE.
4. Refactor `/backend/app/api/v1/insights.py` (for `/insights/generate`) to use the same `LLMClient` instead of direct OpenAI calls.

Frontend TODOs:

- No major changes; keep using existing SSE logic and chat UI components.

### Phase 2 – RAG Layer

**Goal:** Allow the agent to use an internal knowledge base instead of ever-longer prompts.

Backend TODOs:

1. Add `ai_kb_documents` table (and embedding index) via Alembic migration.
2. Implement an internal service (e.g. `backend/app/agent/rag.py`) that:
   - Embeds a query using an embedding model,
   - Queries `ai_kb_documents` via pgvector,
   - Returns top K snippets for prompts.
3. Update `/chat/send` and/or `/insights/generate` to:
   - Call RAG helper before LLM,
   - Include relevant snippets in the system/assistant messages.

Optional admin endpoints:

- `POST /api/v1/agent/kb/index` – bulk ingest docs.
- `GET /api/v1/agent/kb/search` – preview RAG results.

### Phase 3 – Logging, Feedback, and Learning

**Goal:** Use real user interactions to improve behavior over time.

Backend TODOs:

1. Ensure all `agent.CONVERSATIONS` and `agent.MESSAGES` fields needed for analysis are being populated (mode, provider, tools used, etc.).
2. Add `ai_feedback` table and an endpoint like `POST /api/v1/agent/messages/{id}/feedback`.
3. Create a simple “AI analytics” module or script that:
   - Reads `agent.MESSAGES`, `AI_INSIGHTS`, `ai_feedback`,
   - Groups by semantically similar questions (via embeddings),
   - Produces reports (e.g. top failing topics).

Frontend TODOs:

1. Add feedback UI (thumbs up/down + optional text) to chat messages and/or insights.
2. Wire these controls to the new feedback endpoint.

### Phase 4 – Multi‑Provider & Advanced Routing (Optional)

**Goal:** Be able to plug in Claude or other providers easily.

Backend TODOs:

1. Implement `/backend/app/agent/llm_claude.py` (if/when you add Anthropic).
2. Update `get_llm_client()` to choose a provider based on:
   - env var (`SIGMASIGHT_AI_PROVIDER`),
   - conversation mode,
   - feature flags, or
   - task type (e.g. long‑form research).
3. Add lightweight routing logic for heavy vs light tasks:
   - Default: `gpt-5-mini`
   - Heavy analysis: `gpt-5.1`
   - Specialized: possibly Claude Sonnet/Opus

Frontend:

- Largely unchanged; it still talks to `/chat/send` via SSE.


---

## 6. How This Ties Back To The Other AgentCoPilot Docs

- **`SIGMASIGHT_AGENT_OVERVIEW.md`** – Roles, repo layout, global rules.
- **`SIGMASIGHT_AGENT_BACKEND.md`** – Detailed backend rules, endpoints, and tools.
- **`SIGMASIGHT_AGENT_FRONTEND.md`** – Frontend integration, SSE, and UI context.
- **`SIGMASIGHT_AGENT_TOOLS_AND_DATA.md`** – Data contracts and anti‑hallucination rules.
- **`SIGMASIGHT_AGENT_CONTEXT_AND_LEARNING.md`** – RAG, memories, feedback schemas.
- **`SIGMASIGHT_AGENT_WORKFLOW.md`** – Day‑to‑day workflow and quality checklist.
- **`SIGMASIGHT_AGENT_EXECUTION_PLAN.md`** (this file) – Concrete execution roadmap and provider strategy.

AI coding agents should read all of these, but use this execution plan as the **“what do I build next?”** guide.
